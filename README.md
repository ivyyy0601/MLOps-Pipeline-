# üé¨ MLOps-Pipeline: YouTube Sentiment Analysis

End-to-end **YouTube comment sentiment analysis MLOps** project:  
Data ‚Üí Preprocessing/EDA ‚Üí Baseline Model ‚Üí MLflow (AWS) ‚Üí DVC Reproducible Pipeline ‚Üí Model Registry ‚Üí Flask API ‚Üí Chrome Extension ‚Üí Docker & GitHub Actions CI/CD.

---

## 1Ô∏è‚É£ Data Collection
Collect raw data via **APIs / databases / CSV** (this project uses CSV).
```bash
dvc repro   # first run triggers data_ingestion
```

---

## 2Ô∏è‚É£ Data Preprocessing & EDA
- **Cleaning & Processing**: handle missing/duplicate values, regex cleaning, lowercasing  
- **Stopwords & Text**: custom stopword list, keep negation/transition words, lemmatization  
- **Features**: BoW/TF‚ÄëIDF, control n‚Äëgram and max_features  
- **Stats**: word/char/punctuation counts  
- **EDA**: class balance, text length distribution, stopword stats, Top‚ÄëN n‚Äëgrams, **word cloud & frequency analysis**

**Outputs**
- `data/interim/train_processed.csv`  
- `data/interim/test_processed.csv`

---

## 3Ô∏è‚É£ Baseline Model
First make it work with a simple baseline: **TF‚ÄëIDF + Logistic Regression / Random Forest**. Use metrics as a reference for later improvements.
```bash
dvc repro   # triggers model_building & model_evaluation
```
**Artifacts**
- `lgbm_model.pkl`  
- `tfidf_vectorizer.pkl`  
- `experiment_info.json`

> Steps 1‚Äì3 were validated in `notebooks/1_Preprocessing_&_EDA.ipynb`.

---

## 4Ô∏è‚É£ MLflow Server Setup on AWS
**Purpose**: Centralized experiment tracking (logs, parameters, metrics, model versioning).

**Components**
- **IAM User** (e.g., `ml_server`) with S3 access  
- **S3 Bucket** (e.g., `mlbucket922`) to store MLflow **artifacts** (models/logs/files)  
- **EC2 Instance** (e.g., `ml-machine`) to run the **MLflow server**

### ‚öôÔ∏è AWS Setup Details (in order)
**1) Connect to EC2**
```bash
sudo apt update
sudo apt install python3-pip
sudo apt install pipenv
sudo apt install virtualenv
```

**2) Project Environment**
```bash
mkdir mlflow && cd mlflow
pipenv install mlflow awscli boto3
pipenv shell
aws configure
```

**3) Run MLflow Server**
```bash
mlflow server \
    --host 0.0.0.0 \
    --default-artifact-root s3://mlbucket922
# replace mlbucket922 with your S3 bucket name
```

**4) Access MLflow UI**
- Add **inbound port 5000** in the EC2 **Security Group**  
- Open: `http://<EC2-Public-DNS>:5000/`  
  (example: `http://ec2-xx-xxx-xxx-xx.compute-1.amazonaws.com:5000/` ‚Äî use your own)

> The four steps above mirror the internal doc sequence. Follow them to bring MLflow online.

---

## 5Ô∏è‚É£ Improve Baseline Model (Experiment Plan)
In NLP, a typical baseline is **BoW/TF‚ÄëIDF + LogReg/SVM/Naive Bayes**. Improvement path:
- **BoW / TF‚ÄëIDF**: fundamental text vectorization
- **Max Features**: cap vocabulary size (e.g., 5k) to control complexity and speed/generalization
- **Imbalanced Data**: SMOTE, class weights, undersampling
- **Multiple Models + Tuning**: LogReg / RF / XGBoost / SVM; Grid/Random/Optuna
- **Stacking**: use base model predictions as features for a meta‚Äëmodel

**Notebooks (already in repo)**
- Baseline: `2_experiment_1_baseline_model.ipynb`  
- BoW vs TF‚ÄëIDF: `3_experiment_2_bow_tfidf.ipynb`  
- TF‚ÄëIDF + n‚Äëgram + max_features: `4_experiment_3_tfidf_(1,3)_max_features.ipynb`  
- Imbalanced: `5_experiment_4_handling_imbalanced_data.ipynb`  
- XGBoost + HPT: `6_experiment_5_xgboost_with_hpt.ipynb`  
- LightGBM + HPT: `7_experiment_6_lightgbm_detailed_hpt.ipynb`  
- Stacking: `8_stacking.ipynb`

> All experiments are tracked in **MLflow**.

---

## 6Ô∏è‚É£ Make It Reproducible with DVC
`dvc.yaml` splits the pipeline into **4+1** stages (the last one is registration):
- **data_ingestion** ‚Üí read/split raw data (controlled by `data_ingestion.test_size`)
- **data_preprocessing** ‚Üí cleaning + vectorization (controlled by `ngram_range / max_features / vectorizer`)
- **model_building** ‚Üí training (controlled by LightGBM/XGBoost/LogReg params, etc.)
- **model_evaluation** ‚Üí unified evaluation, outputs `experiment_info.json`
- **model_registration** ‚Üí (see next) write into MLflow Model Registry

**Commands**
```bash
dvc repro          # reproduce the whole pipeline
dvc exp run        # run an experiment with parameter changes
dvc params diff    # show parameter differences
```

---

## 7Ô∏è‚É£ Register the Model (MLflow Model Registry)
**Goal**: Register the **approved best model** into MLflow Registry with a canonical name & versioning for easy deployment/rollback.

**Script & Dependencies**
- `src/model/register_model.py`  
- **Input**: `experiment_info.json` (tells the script where to load the best artifacts)  
- **Output**: `model_registration.json` (records model name & version)

**Run**
```bash
python src/model/register_model.py
# or trigger via the model_registration stage in dvc.yaml
```
**Result**: In MLflow, you‚Äôll see versions (e.g., `yt_chrome_plugin_model` v1/v2/‚Ä¶).

---

## 8Ô∏è‚É£ Serving with Flask API
**Location**: `flask_api/app.py`  
**Highlights**: CORS enabled; supports loading from **MLflow Model Registry** (or fallback to local `pkl`).

**Key Endpoints**
- **POST /predict**  
  Request: `{"comments": ["text1", "text2", ...]}`  
  Response: `[{"comment": "...", "sentiment": 1|0|-1}, ...]`
- **POST /predict_with_timestamps**  
  Request: `{"comments":[{"text":"...","timestamp":"01:23"}, ...]}`  
  Response: predictions with timestamps for video overlay/markers
- **POST /generate_chart**  
  Request: `{"sentiment_counts":{"1":10,"0":5,"-1":2}}`  
  Response: pie chart PNG (Matplotlib)
- **POST /generate_wordcloud**  
  Request: `{"comments":["...","..."]}`  
  Response: word cloud PNG (WordCloud)

**Run locally**
```bash
python flask_api/app.py  # default 0.0.0.0:5000
```

**Example (cURL)**
```bash
curl -X POST http://localhost:5000/predict   -H "Content-Type: application/json"   -d '{"comments":["this is great","not good at all"]}'
```

---

## 9Ô∏è‚É£ Frontend: Chrome Extension (`yt-chrome-plugin-frontend/`)
**Suggested Structure**
```
yt-chrome-plugin-frontend/
‚îú‚îÄ manifest.json
‚îú‚îÄ popup.html
‚îú‚îÄ popup.js
‚îú‚îÄ background.js         # optional: long-running tasks / context menus
‚îî‚îÄ icons/                # 16/48/128 px
```

**Workflow**
1. `popup.js` reads comments from the page or via **YouTube Data API** (configure API key, or fetch via backend).  
2. POST the comment array to Flask `/predict` or `/predict_with_timestamps`.  
3. Render sentiment labels and ratios; optionally fetch `/generate_chart` and `/generate_wordcloud` images and display.

**Load for development**
- Open `chrome://extensions/` ‚Üí enable **Developer mode** ‚Üí **Load unpacked** ‚Üí select `yt-chrome-plugin-frontend/`  
- In the extension settings, set **API Base URL** (e.g., `http://<your-domain-or-ec2>:5000`)

**Security tip**: Store your API key in `chrome.storage` or the backend; **don‚Äôt hardcode it in Git**.

---

## üîü Dockerization & Deployment (ECR + EC2)
**Build image**
```bash
docker build -t mlops-pipeline:latest .
```

**Run (locally or on EC2)**
```bash
docker run -d -p 8080:5000   -e MLFLOW_TRACKING_URI="http://<mlflow-ec2>:5000"   --name yt-sentiment mlops-pipeline:latest
```

**Push to ECR (example)**
```bash
aws ecr get-login-password --region us-east-1 |   docker login --username AWS --password-stdin <acct>.dkr.ecr.us-east-1.amazonaws.com

docker tag mlops-pipeline:latest <acct>.dkr.ecr.us-east-1.amazonaws.com/mlops-pipeline:latest
docker push <acct>.dkr.ecr.us-east-1.amazonaws.com/mlops-pipeline:latest
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ GitHub Actions: CI/CD (with Self-Hosted Runner)
**Required Secrets**
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION` (e.g., `us-east-1`)
- `AWS_ECR_LOGIN_URI` (e.g., `<acct>.dkr.ecr.us-east-1.amazonaws.com`)
- `ECR_REPOSITORY_NAME` (e.g., `mlops-pipeline`)

**Recommended Stages**
- **CI**: Checkout ‚Üí Lint ‚Üí Unit tests  
- **CD (build & push to ECR)**: Login ECR ‚Üí Build ‚Üí Push  
- **Deploy (on self-hosted runner/EC2)**: Pull ‚Üí `docker run -d -p 8080:5000 ...`

---

### Notes
- For macOS + LightGBM/OpenMP errors: `brew install libomp` and export `DYLD_LIBRARY_PATH` accordingly.  
- Keep secrets out of the repo; never commit real API keys or credentials.  
- `experiment_info.json` links training/evaluation outputs; `model_registration.json` records the production model name/version.
